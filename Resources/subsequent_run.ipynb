{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from config import id_1, id_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent Run Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv and get the songs that's not in the set\n",
    "former_song_df = pd.read_csv('../data/ochalady-sakusaku.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Date_Created</th>\n",
       "      <th>Title</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Invite_Spawner</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>854161284_4209808395</td>\n",
       "      <td>2021-11-14</td>\n",
       "      <td>水平線</td>\n",
       "      <td>back number</td>\n",
       "      <td>saku2222saku</td>\n",
       "      <td>https://www.smule.com/recording/back-number-%E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>854161284_4150283706</td>\n",
       "      <td>2021-09-12</td>\n",
       "      <td>ルカルカ★ナイトフィーバー／ボカロ</td>\n",
       "      <td>Luka Megurine</td>\n",
       "      <td>saku2222saku</td>\n",
       "      <td>https://www.smule.com/recording/luka-megurine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>854161284_4143509048</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>夜に駆ける (-5)</td>\n",
       "      <td>YOASOBI</td>\n",
       "      <td>saku2222saku</td>\n",
       "      <td>https://www.smule.com/recording/yoasobi-%E5%A4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>854161284_4130371806</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>チェリー</td>\n",
       "      <td>スピッツ</td>\n",
       "      <td>saku2222saku</td>\n",
       "      <td>https://www.smule.com/recording/%E3%82%B9%E3%8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>854161284_4129357684</td>\n",
       "      <td>2021-08-22</td>\n",
       "      <td>Kokonishika Sakanai Hana - ここにしか咲かない花 小渕･黒田ﾊﾟｰ...</td>\n",
       "      <td>コブクロ</td>\n",
       "      <td>saku2222saku</td>\n",
       "      <td>https://www.smule.com/recording/%E3%82%B3%E3%8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>866011258_988543101</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>Sword Art Online - Crossing Field</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>OchaLady</td>\n",
       "      <td>https://www.smule.com/recording/lisa-sword-art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>866011258_972287515</td>\n",
       "      <td>2017-01-26</td>\n",
       "      <td>WAVE</td>\n",
       "      <td>niki</td>\n",
       "      <td>OchaLady</td>\n",
       "      <td>https://www.smule.com/recording/niki-wave/8660...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>866011258_964309189</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>夢の続きへ/SURFACE</td>\n",
       "      <td>SURFACE</td>\n",
       "      <td>OchaLady</td>\n",
       "      <td>https://www.smule.com/recording/surface-%E5%A4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>866011258_944253352</td>\n",
       "      <td>2017-01-16</td>\n",
       "      <td>PHANTOM MINDS</td>\n",
       "      <td>Mizuki Nana</td>\n",
       "      <td>OchaLady</td>\n",
       "      <td>https://www.smule.com/recording/mizuki-nana-ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>866011258_916987642</td>\n",
       "      <td>2017-01-07</td>\n",
       "      <td>Regret</td>\n",
       "      <td>Hoshimura Mai</td>\n",
       "      <td>OchaLady</td>\n",
       "      <td>https://www.smule.com/recording/hoshimura-mai-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      key Date_Created  \\\n",
       "0    854161284_4209808395   2021-11-14   \n",
       "1    854161284_4150283706   2021-09-12   \n",
       "2    854161284_4143509048   2021-09-05   \n",
       "3    854161284_4130371806   2021-08-23   \n",
       "4    854161284_4129357684   2021-08-22   \n",
       "..                    ...          ...   \n",
       "170   866011258_988543101   2017-02-01   \n",
       "171   866011258_972287515   2017-01-26   \n",
       "172   866011258_964309189   2017-01-23   \n",
       "173   866011258_944253352   2017-01-16   \n",
       "174   866011258_916987642   2017-01-07   \n",
       "\n",
       "                                                 Title         Artist  \\\n",
       "0                                                  水平線    back number   \n",
       "1                                    ルカルカ★ナイトフィーバー／ボカロ  Luka Megurine   \n",
       "2                                           夜に駆ける (-5)        YOASOBI   \n",
       "3                                                 チェリー           スピッツ   \n",
       "4    Kokonishika Sakanai Hana - ここにしか咲かない花 小渕･黒田ﾊﾟｰ...           コブクロ   \n",
       "..                                                 ...            ...   \n",
       "170                  Sword Art Online - Crossing Field           Lisa   \n",
       "171                                               WAVE           niki   \n",
       "172                                      夢の続きへ/SURFACE        SURFACE   \n",
       "173                                      PHANTOM MINDS    Mizuki Nana   \n",
       "174                                             Regret  Hoshimura Mai   \n",
       "\n",
       "    Invite_Spawner                                                URL  \n",
       "0     saku2222saku  https://www.smule.com/recording/back-number-%E...  \n",
       "1     saku2222saku  https://www.smule.com/recording/luka-megurine-...  \n",
       "2     saku2222saku  https://www.smule.com/recording/yoasobi-%E5%A4...  \n",
       "3     saku2222saku  https://www.smule.com/recording/%E3%82%B9%E3%8...  \n",
       "4     saku2222saku  https://www.smule.com/recording/%E3%82%B3%E3%8...  \n",
       "..             ...                                                ...  \n",
       "170       OchaLady  https://www.smule.com/recording/lisa-sword-art...  \n",
       "171       OchaLady  https://www.smule.com/recording/niki-wave/8660...  \n",
       "172       OchaLady  https://www.smule.com/recording/surface-%E5%A4...  \n",
       "173       OchaLady  https://www.smule.com/recording/mizuki-nana-ph...  \n",
       "174       OchaLady  https://www.smule.com/recording/hoshimura-mai-...  \n",
       "\n",
       "[175 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up with deleting extra column\n",
    "former_song_df = former_song_df.drop(columns=['Unnamed: 0'])\n",
    "former_song_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_key = set(former_song_df['key'].tolist())\n",
    "# len(set_key)\n",
    "# set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Song, Date, URL, inviter\n",
    "# key = []\n",
    "# title = []\n",
    "# artist = []\n",
    "# date = []\n",
    "# inviter = []\n",
    "# song_url = []\n",
    "# recording_base_url = 'https://www.smule.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building url\n",
    "user_1 = f'https://www.smule.com/s/profile/performance/{id_1}/sing?offset='\n",
    "user_2 = f'https://www.smule.com/s/profile/performance/{id_2}/sing?offset='\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store all the data output\n",
    "mining_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(f'{user_1}{offset}')\n",
    "# response_json = response.json()\n",
    "# json_stuff = response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for while loop\n",
    "n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_1 is the VIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response code: 418\n",
      "Unsuccessful Request T^T\n"
     ]
    }
   ],
   "source": [
    "# DOES THIS SH** EVEN WORK!!?? Can't TEST IT b/c G** D*** TEAPOT!\n",
    "# Infinite loop until the list is empty (offset ran out)\n",
    "\n",
    "while True:\n",
    "    response = requests.get(f'{user_2}{offset}')\n",
    "    print(f'response code: {response.status_code}')\n",
    "    # Get out of the loop if request is unsuccessful\n",
    "    if response.status_code != 200:\n",
    "        print('Unsuccessful Request T^T')\n",
    "        break\n",
    "    response_json = response.json()\n",
    "    json_stuff = response_json['list']\n",
    "    n += 1\n",
    "    print(f'Request #{n}: Success!')\n",
    "    \n",
    "    # If the list is empty in the json data\n",
    "    if not json_stuff:\n",
    "        break\n",
    "\n",
    "    # Set the condition for loop to break when everything is retrieved\n",
    "    break_point = False\n",
    "    \n",
    "    for x in range(0,len(json_stuff)):\n",
    "        if json_stuff[x]['key'] not in set_key: # this probably needs to be updated..? i don't know anymore\n",
    "            if json_stuff[x]['performed_by'] == id_1:\n",
    "                mining_list.append(json_stuff[x])\n",
    "                print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "\n",
    "        else: # Add this line and indent for subsequent run only\n",
    "            print('All data has already been extracted') # Add this line and indent for subsequent run only\n",
    "            break_point = True\n",
    "            break # Add this line and indent for subsequent run only\n",
    "\n",
    "    # Get out of the data request if break_point is True\n",
    "    if break_point:\n",
    "        break\n",
    "    \n",
    "    # Otherwise keep going\n",
    "    offset = response_json['next_offset']\n",
    "    \n",
    "    if offset == -1:\n",
    "        print(f'No more records to be processed. Total Loops: {n}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Infinite loop until the list is empty (offset ran out)\n",
    "\n",
    "# while True:\n",
    "#     response = requests.get(f'{user_2}{offset}')\n",
    "#     # Get out of the loop if request is unsuccessful\n",
    "#     if response.status_code != 200:\n",
    "#         print('Unsuccessful Request T^T')\n",
    "#         break\n",
    "#     response_json = response.json()\n",
    "#     json_stuff = response_json['list']\n",
    "    \n",
    "#     # If the list is empty in the json data\n",
    "#     if not json_stuff:\n",
    "#         break\n",
    "    \n",
    "#     for x in range(0,len(json_stuff)):\n",
    "# #         if json_stuff[x]['key'] not in set_key: # Add this line and indent for subsequent run only\n",
    "#         if json_stuff[x]['performed_by'] == id_1:\n",
    "#             key.append(json_stuff[x]['key'])\n",
    "#             inviter.append(json_stuff[x]['performed_by'])\n",
    "#             title.append(json_stuff[x]['title'])\n",
    "#             artist.append(json_stuff[x]['artist'])\n",
    "#             date.append(json_stuff[x]['created_at'])\n",
    "#             recording_url = recording_base_url + json_stuff[x]['web_url']\n",
    "#             song_url.append(recording_url)\n",
    "#             print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "\n",
    "# #         else: # Add this line and indent for subsequent run only\n",
    "# #             print('All data has already been extracted') # Add this line and indent for subsequent run only\n",
    "# #             break # Add this line and indent for subsequent run only\n",
    "            \n",
    "#     offset = response_json['next_offset']\n",
    "    \n",
    "#     if offset == -1:\n",
    "#         print('No more records to be processed')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapped all the wanted field from the data mining + rename field\n",
    "def create_song_list(track):\n",
    "    key = track['key']\n",
    "    title = track['title']\n",
    "    artist = track['artist']\n",
    "    date = track['created_at']\n",
    "    inviter = track['performed_by']\n",
    "    recording_base_url = 'https://www.smule.com'\n",
    "    recording_url = recording_base_url + track['web_url']\n",
    "    \n",
    "    return {\n",
    "        'key': key,\n",
    "        'Title': title,\n",
    "        'Artist': artist,\n",
    "        'Date_Created': date,\n",
    "        'Invite_Spawner': inviter,\n",
    "        'URL': recording_url\n",
    "    }\n",
    "    \n",
    "# Use map to give a list iterator\n",
    "song_list_iterator = map(create_song_list, mining_list)\n",
    "# Turn iterator into a list\n",
    "song_list = list(song_list_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To dataframe and transform date string\n",
    "id2_songs_add_df = pd.DataFrame(song_list)\n",
    "id2_songs_add_df['Date_Created'] = id2_songs_add_df['Date_Created'].str.split('T').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save first result to csv\n",
    "id2_songs_add_df.to_csv('../data/id2_songs_add.csv', encoding='utf_8_sig', index=False)\n",
    "id2_songs_add_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_2 is the VIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the above mapping function works, overhaul this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run another infinite loop again - You might have to wait 30mins b/c you'll get 418 code :P\n",
    "\n",
    "while True:\n",
    "    response = requests.get(f'{user_1}{offset}')\n",
    "    # Get out of the loop if request is unsuccessful\n",
    "    if response.status_code != 200:\n",
    "        print('Unsuccessful Request T^T')\n",
    "        break\n",
    "    response_json = response.json()\n",
    "    json_stuff = response_json['list']\n",
    "    \n",
    "    # If the list is empty in the json data\n",
    "    if not json_stuff:\n",
    "        break\n",
    "    \n",
    "    for x in range(0,len(json_stuff)):\n",
    "#         if json_stuff[x]['key'] not in set_key: # Add this line and indent for subsequent run only\n",
    "        if json_stuff[x]['performed_by'] == id_2:\n",
    "            key.append(json_stuff[x]['key'])\n",
    "            inviter.append(json_stuff[x]['performed_by'])\n",
    "            title.append(json_stuff[x]['title'])\n",
    "            artist.append(json_stuff[x]['artist'])\n",
    "            date.append(json_stuff[x]['created_at'])\n",
    "            recording_url = recording_base_url + json_stuff[x]['web_url']\n",
    "            song_url.append(recording_url)\n",
    "            print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "#         else: # Add this line and indent for subsequent run only\n",
    "#             print('All data has already been extracted') # Add this line and indent for subsequent run only\n",
    "#             break # Add this line and indent for subsequent run only\n",
    "    offset = response_json['next_offset']\n",
    "    \n",
    "    if offset == -1:\n",
    "        print('No more records to be processed')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe and save the first data request into csv\n",
    "date_created = []\n",
    "\n",
    "for d in date:\n",
    "    date_only = d.split('T')[0]\n",
    "    date_created.append(date_only)\n",
    "\n",
    "# Show the list of songs in dataframe for id_2\n",
    "id1_songs_add_df = pd.DataFrame({'key': key,\n",
    "                             'Date_Created': date_created,\n",
    "                             'Title':title,\n",
    "                             'Artist': artist,\n",
    "                             'Invite_Spawner': inviter,\n",
    "                             'URL': song_url})\n",
    "\n",
    "# Save first result to csv\n",
    "id1_songs_add_df.to_csv('../data/id1_songs_add.csv', encoding='utf_8_sig', index=False)\n",
    "id1_songs_add_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the two csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import id2_songs.csv\n",
    "id2_songs_add_df = pd.read_csv('../data/id2_songs_add.csv')\n",
    "id2_songs_add_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHEN DATA IS RETRIEVED, CHECK IF THERE'S EXTRA INDEX AND REMOVE OR DO WHATEVER NECESSARY TO APPEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the two dataframes\n",
    "all_songs_df = id1_songs_add_df.append(id2_songs_add_df)\n",
    "\n",
    "# Append to the original dataframe\n",
    "all_songs_df = all_songs_df.append(former_song_df)\n",
    "\n",
    "# Sort the df by date\n",
    "all_songs_df = all_songs_df.sort_values('Date_Created', ascending=False)\n",
    "\n",
    "# Reset index\n",
    "all_songs_df.reset_index(inplace=True)\n",
    "\n",
    "# Delete old index\n",
    "del all_songs_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result\n",
    "print(f'{id_1} joined {id_2} on {len(id1_songs_add_df)} additional songs!')\n",
    "print(f'{id_2} joined {id_1} on {len(id2_songs_add_df)} additional songs!')\n",
    "print(f'You guys have made {len(all_songs_df)} recordings together as of this date.')\n",
    "\n",
    "all_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv with encoding that allows foreign character\n",
    "# all_songs_df.to_csv('../data/ochalady-sakusaku.csv', encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only if both runs go smoothly, run the code below and skip above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up & Export -- THIS NEEDS TO BE CLEANED UP -- ALSO LET BE REAL, WE NEVER GONNA BE ABLE TO DO THIS CUZ THE D*** TEAPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the date column\n",
    "# date_created = []\n",
    "\n",
    "# for d in date:\n",
    "#     date_only = d.split('T')[0]\n",
    "#     date_created.append(date_only)\n",
    "    \n",
    "# # Show the list of songs in dataframe\n",
    "# all_songs_df = pd.DataFrame({'key': key,\n",
    "#                              'Date_Created': date_created,\n",
    "#                              'Title':title,\n",
    "#                              'Artist': artist,\n",
    "#                              'Invite_Spawner': inviter,\n",
    "#                              'URL': song_url})\n",
    "\n",
    "# # Sort the df by date\n",
    "# all_songs_df = all_songs_df.sort_values('Date Created')\n",
    "\n",
    "# # Reset index\n",
    "# all_songs_df.reset_index(inplace=True)\n",
    "\n",
    "# # Delete old index\n",
    "# del all_songs_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ta..da...\n",
    "print(f'{id_1} joined {id_2} on {len(id1_songs_add_df)} additional songs!')\n",
    "print(f'{id_2} joined {id_1} on {len(id2_songs_add_df)} additional songs!')\n",
    "print(f'You guys have made {len(all_songs_df)} recordings together as of this date.')\n",
    "\n",
    "all_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv with encoding that allows foreign character\n",
    "# all_songs_df.to_csv('ochalady-sakusaku.csv', encoding='utf_8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
