{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from config import id_1, id_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent Run Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv and get the songs that's not in the set\n",
    "song_df = pd.read_csv('../data/ochalady-sakusaku.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_key = set(song_df['key'].tolist())\n",
    "len(set_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Song, Date, URL, inviter\n",
    "key = []\n",
    "title = []\n",
    "artist = []\n",
    "date = []\n",
    "inviter = []\n",
    "song_url = []\n",
    "recording_base_url = 'https://www.smule.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building url\n",
    "user_1 = f'https://www.smule.com/s/profile/performance/{id_1}/sing?offset='\n",
    "user_2 = f'https://www.smule.com/s/profile/performance/{id_2}/sing?offset='\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_1 is the VIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite loop until the list is empty (offset ran out)\n",
    "\n",
    "while True:\n",
    "    response = requests.get(f'{user_2}{offset}')\n",
    "    # Get out of the loop if request is unsuccessful\n",
    "    if response.status_code != 200:\n",
    "        print('Unsuccessful Request T^T')\n",
    "        break\n",
    "    response_json = response.json()\n",
    "    json_stuff = response_json['list']\n",
    "    \n",
    "    # If the list is empty in the json data\n",
    "    if not json_stuff:\n",
    "        break\n",
    "    \n",
    "    for x in range(0,len(json_stuff)):\n",
    "#         if json_stuff[x]['key'] not in set_key: # Add this line and indent for subsequent run only\n",
    "        if json_stuff[x]['performed_by'] == id_1:\n",
    "            key.append(json_stuff[x]['key'])\n",
    "            inviter.append(json_stuff[x]['performed_by'])\n",
    "            title.append(json_stuff[x]['title'])\n",
    "            artist.append(json_stuff[x]['artist'])\n",
    "            date.append(json_stuff[x]['created_at'])\n",
    "            recording_url = recording_base_url + json_stuff[x]['web_url']\n",
    "            song_url.append(recording_url)\n",
    "            print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "\n",
    "#         else: # Add this line and indent for subsequent run only\n",
    "#             print('All data has already been extracted') # Add this line and indent for subsequent run only\n",
    "#             break # Add this line and indent for subsequent run only\n",
    "            \n",
    "    offset = response_json['next_offset']\n",
    "    \n",
    "    if offset == -1:\n",
    "        print('No more records to be processed')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe and save the first data request into csv\n",
    "date_created = []\n",
    "\n",
    "for d in date:\n",
    "    date_only = d.split('T')[0]\n",
    "    date_created.append(date_only)\n",
    "\n",
    "# Show the list of songs in dataframe for id_2\n",
    "id2_songs_df = pd.DataFrame({'key': key,\n",
    "                             'Date_Created': date_created,\n",
    "                             'Title':title,\n",
    "                             'Artist': artist,\n",
    "                             'Invite_Spawner': inviter,\n",
    "                             'URL': song_url})\n",
    "\n",
    "# Save first result to csv\n",
    "id2_songs_df.to_csv('../data/id2_songs_df.csv', encoding='utf_8_sig', index=False)\n",
    "id2_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many songs did id_2 join id_1 in total?\n",
    "join_id1 = len(title)\n",
    "join_id1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_2 is the VIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run another infinite loop again - You might have to wait 30mins b/c you'll get 418 code :P\n",
    "\n",
    "while True:\n",
    "    response = requests.get(f'{user_1}{offset}')\n",
    "    # Get out of the loop if request is unsuccessful\n",
    "    if response.status_code != 200:\n",
    "        print('Unsuccessful Request T^T')\n",
    "        break\n",
    "    response_json = response.json()\n",
    "    json_stuff = response_json['list']\n",
    "    \n",
    "    # If the list is empty in the json data\n",
    "    if not json_stuff:\n",
    "        break\n",
    "    \n",
    "    for x in range(0,len(json_stuff)):\n",
    "#         if json_stuff[x]['key'] not in set_key: # Add this line and indent for subsequent run only\n",
    "        if json_stuff[x]['performed_by'] == id_2:\n",
    "            key.append(json_stuff[x]['key'])\n",
    "            inviter.append(json_stuff[x]['performed_by'])\n",
    "            title.append(json_stuff[x]['title'])\n",
    "            artist.append(json_stuff[x]['artist'])\n",
    "            date.append(json_stuff[x]['created_at'])\n",
    "            recording_url = recording_base_url + json_stuff[x]['web_url']\n",
    "            song_url.append(recording_url)\n",
    "            print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "#         else: # Add this line and indent for subsequent run only\n",
    "#             print('All data has already been extracted') # Add this line and indent for subsequent run only\n",
    "#             break # Add this line and indent for subsequent run only\n",
    "    offset = response_json['next_offset']\n",
    "    \n",
    "    if offset == -1:\n",
    "        print('No more records to be processed')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe and save the first data request into csv\n",
    "date_created = []\n",
    "\n",
    "for d in date:\n",
    "    date_only = d.split('T')[0]\n",
    "    date_created.append(date_only)\n",
    "\n",
    "# Show the list of songs in dataframe for id_2\n",
    "id1_songs_df = pd.DataFrame({'key': key,\n",
    "                             'Date_Created': date_created,\n",
    "                             'Title':title,\n",
    "                             'Artist': artist,\n",
    "                             'Invite_Spawner': inviter,\n",
    "                             'URL': song_url})\n",
    "\n",
    "# Save first result to csv\n",
    "id1_songs_df.to_csv('../data/id1_songs.csv', encoding='utf_8_sig', index=False)\n",
    "id1_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many songs did id_1 join id_2 in total?\n",
    "join_id2 = len(title)\n",
    "join_id2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the two csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import id2_songs.csv\n",
    "id2_songs_df = pd.read_csv('../data/id2_songs.csv')\n",
    "id2_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the two dataframes\n",
    "all_songs_df = id1_songs_df.append(id2_songs_df)\n",
    "\n",
    "# Sort the df by date\n",
    "all_songs_df = all_songs_df.sort_values('Date_Created', ascending=False)\n",
    "\n",
    "# Reset index\n",
    "all_songs_df.reset_index(inplace=True)\n",
    "\n",
    "# Delete old index\n",
    "del all_songs_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result\n",
    "print(f'{id_1} joined {id_2} on {len(id1_songs_df)} songs!')\n",
    "print(f'{id_2} joined {id_1} on {len(id2_songs_df)} songs!')\n",
    "print(f'You guys have made {len(all_songs_df)} recordings together as of this date.')\n",
    "\n",
    "all_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv with encoding that allows foreign character\n",
    "all_songs_df.to_csv('../data/ochalady-sakusaku.csv', encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only if both runs go smoothly, run the code below and skip above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the date column\n",
    "date_created = []\n",
    "\n",
    "for d in date:\n",
    "    date_only = d.split('T')[0]\n",
    "    date_created.append(date_only)\n",
    "    \n",
    "# Show the list of songs in dataframe\n",
    "all_songs_df = pd.DataFrame({'key': key,\n",
    "                             'Date_Created': date_created,\n",
    "                             'Title':title,\n",
    "                             'Artist': artist,\n",
    "                             'Invite_Spawner': inviter,\n",
    "                             'URL': song_url})\n",
    "\n",
    "# Sort the df by date\n",
    "all_songs_df = all_songs_df.sort_values('Date Created')\n",
    "\n",
    "# Reset index\n",
    "all_songs_df.reset_index(inplace=True)\n",
    "\n",
    "# Delete old index\n",
    "del all_songs_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ta-da!\n",
    "print(f'{id_1} joined {id_2} on {join_id2} songs!')\n",
    "print(f'{id_2} joined {id_1} on {join_id1} songs!')\n",
    "print(f'You guys have made {len(songs)} recordings together as of this date.')\n",
    "\n",
    "all_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv with encoding that allows foreign character\n",
    "all_songs_df.to_csv('ochalady-sakusaku.csv', encoding='utf_8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
