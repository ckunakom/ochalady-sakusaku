{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from config import id_1, id_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent Run Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv and get the songs that's not in the set\n",
    "former_song_df = pd.read_csv('../data/ochalady-sakusaku.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up with deleting extra column\n",
    "former_song_df = former_song_df.drop(columns=['Unnamed: 0'])\n",
    "# former_song_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_key = set(former_song_df['key'].tolist())\n",
    "# len(set_key)\n",
    "# set_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building url\n",
    "user_1 = f'https://www.smule.com/s/profile/performance/{id_1}/sing?offset='\n",
    "user_2 = f'https://www.smule.com/s/profile/performance/{id_2}/sing?offset='\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store all the data output\n",
    "mining_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for while loop\n",
    "n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_1 is the VIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite loop until the list is empty (offset ran out)\n",
    "\n",
    "while True:\n",
    "    response = requests.get(f'{user_2}{offset}')\n",
    "    print(f'response code: {response.status_code}')\n",
    "    # Get out of the loop if request is unsuccessful\n",
    "    if response.status_code != 200:\n",
    "        print('Unsuccessful Request T^T')\n",
    "        break\n",
    "    response_json = response.json()\n",
    "    json_stuff = response_json['list']\n",
    "    n += 1\n",
    "    print(f'Request #{n}: Success!')\n",
    "    print(f\"next offset: {response_json['next_offset']}\")\n",
    "    \n",
    "    # If the list is empty in the json data\n",
    "    if not json_stuff:\n",
    "        break\n",
    "\n",
    "    # Set the condition for loop to break when everything is retrieved\n",
    "    break_point = False\n",
    "    \n",
    "    for x in range(0,len(json_stuff)):\n",
    "        # if key is in the set of keys, add to the mining_list\n",
    "        if json_stuff[x]['key'] not in set_key: \n",
    "            if json_stuff[x]['performed_by'].upper() == id_1.upper():\n",
    "                mining_list.append(json_stuff[x])\n",
    "                print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "\n",
    "        # Otherwise, break the loop\n",
    "        else:\n",
    "            print('All data has already been extracted')\n",
    "            break_point = True\n",
    "            break # Add this line and indent for subsequent run only\n",
    "\n",
    "    # Get out of the data request if break_point is True\n",
    "    if break_point:\n",
    "        print(f'No more new records to be processed. Total Loops: {n}')\n",
    "        break\n",
    "    \n",
    "    # Otherwise keep going\n",
    "    offset = response_json['next_offset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapped all the wanted field from the data mining + rename field\n",
    "def create_song_list(track):\n",
    "    key = track['key']\n",
    "    title = track['title']\n",
    "    artist = track['artist']\n",
    "    date = track['created_at']\n",
    "    inviter = track['performed_by']\n",
    "    recording_base_url = 'https://www.smule.com'\n",
    "    recording_url = recording_base_url + track['web_url']\n",
    "    \n",
    "    return {\n",
    "        'key': key,\n",
    "        'Title': title,\n",
    "        'Artist': artist,\n",
    "        'Date_Created': date,\n",
    "        'Invite_Spawner': inviter,\n",
    "        'URL': recording_url\n",
    "    }\n",
    "    \n",
    "# Use map to give a list iterator\n",
    "song_list_iterator = map(create_song_list, mining_list)\n",
    "# Turn iterator into a list\n",
    "song_list = list(song_list_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To dataframe and transform date string\n",
    "id2_songs_add_df = pd.DataFrame(song_list)\n",
    "id2_songs_add_df['Date_Created'] = id2_songs_add_df['Date_Created'].str.split('T').str[0]\n",
    "id2_songs_add_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save first result to csv\n",
    "id2_songs_add_df.to_csv('../data/id2_songs_add.csv', encoding='utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_2 is the VIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWITCH IP ADDRESS WITH VPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get VPN - Use a different IP Address\n",
    "\n",
    "# Reset if uses different VPN to ping data\n",
    "# mining_list = [] \n",
    "# offset = 0\n",
    "# n = 0\n",
    "\n",
    "# Infinite loop until the list is empty (offset ran out)\n",
    "while True:\n",
    "    response = requests.get(f'{user_1}{offset}')\n",
    "    print(f'response code: {response.status_code}')\n",
    "    # Get out of the loop if request is unsuccessful\n",
    "    if response.status_code != 200:\n",
    "        print('Unsuccessful Request T^T')\n",
    "        break\n",
    "    response_json = response.json()\n",
    "    json_stuff = response_json['list']\n",
    "    n += 1\n",
    "    print(f'Request #{n}: Success!')\n",
    "    print(f\"next offset: {response_json['next_offset']}\")\n",
    "    \n",
    "    # If the list is empty in the json data\n",
    "    if not json_stuff:\n",
    "        break\n",
    "\n",
    "    # Set the condition for loop to break when everything is retrieved\n",
    "    break_point = False\n",
    "    \n",
    "    for x in range(0,len(json_stuff)):\n",
    "        # if key is in the set of keys, add to the mining_list\n",
    "        if json_stuff[x]['key'] not in set_key: \n",
    "            if json_stuff[x]['performed_by'].upper() == id_2.upper():\n",
    "                mining_list.append(json_stuff[x])\n",
    "                print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "\n",
    "        # Otherwise, break the loop\n",
    "        else:\n",
    "            print('All data has already been extracted')\n",
    "            break_point = True\n",
    "            break # Add this line and indent for subsequent run only\n",
    "\n",
    "    # Get out of the data request if break_point is True\n",
    "    if break_point:\n",
    "        print(f'No more new records to be processed. Total Loops: {n}')\n",
    "        break\n",
    "    \n",
    "    # Otherwise keep going\n",
    "    offset = response_json['next_offset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapped all the wanted field from the data mining + rename field\n",
    "def create_song_list(track):\n",
    "    key = track['key']\n",
    "    title = track['title']\n",
    "    artist = track['artist']\n",
    "    date = track['created_at']\n",
    "    inviter = track['performed_by']\n",
    "    recording_base_url = 'https://www.smule.com'\n",
    "    recording_url = recording_base_url + track['web_url']\n",
    "    \n",
    "    return {\n",
    "        'key': key,\n",
    "        'Title': title,\n",
    "        'Artist': artist,\n",
    "        'Date_Created': date,\n",
    "        'Invite_Spawner': inviter,\n",
    "        'URL': recording_url\n",
    "    }\n",
    "    \n",
    "# Use map to give a list iterator\n",
    "song_list_iterator = map(create_song_list, mining_list)\n",
    "# Turn iterator into a list\n",
    "song_list = list(song_list_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To dataframe and transform date string\n",
    "id1_songs_add_df = pd.DataFrame(song_list)\n",
    "id1_songs_add_df['Date_Created'] = id1_songs_add_df['Date_Created'].str.split('T').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 2nd result to csv\n",
    "id1_songs_add_df.to_csv('../data/id1_songs_add.csv', encoding='utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the two csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv of both files as df\n",
    "id1_add_df = pd.read_csv('../data/id1_songs_add.csv')\n",
    "id2_add_df = pd.read_csv('../data/id2_songs_add.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to the original dataframe\n",
    "all_songs_df = former_song_df.append(id1_add_df)\n",
    "all_songs_df = all_songs_df.append(id2_add_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the df by date\n",
    "all_songs_df = all_songs_df.sort_values('Date_Created', ascending=False)\n",
    "\n",
    "# Reset index\n",
    "all_songs_df.reset_index(inplace=True)\n",
    "\n",
    "# Delete old index\n",
    "del all_songs_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result\n",
    "print(f'{id_1} joined {id_2} on {len(id1_add_df)} additional songs!')\n",
    "print(f'{id_2} joined {id_1} on {len(id2_add_df)} additional songs!')\n",
    "print(f'You guys have made {len(all_songs_df)} recordings together as of this date.')\n",
    "\n",
    "all_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv with encoding that allows foreign character\n",
    "all_songs_df.to_csv('../data/ochalady-sakusaku.csv', encoding='utf_8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
