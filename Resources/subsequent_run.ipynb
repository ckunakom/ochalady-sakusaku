{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from config import id_1, id_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent Run Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv and get the songs that's not in the set\n",
    "former_song_df = pd.read_csv('../data/ochalady-sakusaku.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up with deleting extra column\n",
    "former_song_df = former_song_df.drop(columns=['Unnamed: 0'])\n",
    "former_song_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_key = set(former_song_df['key'].tolist())\n",
    "# len(set_key)\n",
    "# set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Song, Date, URL, inviter\n",
    "# key = []\n",
    "# title = []\n",
    "# artist = []\n",
    "# date = []\n",
    "# inviter = []\n",
    "# song_url = []\n",
    "# recording_base_url = 'https://www.smule.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building url\n",
    "user_1 = f'https://www.smule.com/s/profile/performance/{id_1}/sing?offset='\n",
    "user_2 = f'https://www.smule.com/s/profile/performance/{id_2}/sing?offset='\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store all the data output\n",
    "mining_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(f'{user_1}{offset}')\n",
    "# response_json = response.json()\n",
    "# json_stuff = response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for while loop\n",
    "n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_1 is the VIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOES THIS SH** EVEN WORK!!?? Can't TEST IT b/c G** D*** TEAPOT!\n",
    "# Infinite loop until the list is empty (offset ran out)\n",
    "\n",
    "while True:\n",
    "    response = requests.get(f'{user_2}{offset}')\n",
    "    print(f'response code: {response.status_code}')\n",
    "    # Get out of the loop if request is unsuccessful\n",
    "    if response.status_code != 200:\n",
    "        print('Unsuccessful Request T^T')\n",
    "        break\n",
    "    response_json = response.json()\n",
    "    json_stuff = response_json['list']\n",
    "    n += 1\n",
    "    print(f'Request #{n}: Success!')\n",
    "    \n",
    "    # If the list is empty in the json data\n",
    "    if not json_stuff:\n",
    "        break\n",
    "\n",
    "    # Set the condition for loop to break when everything is retrieved\n",
    "    break_point = False\n",
    "    \n",
    "    for x in range(0,len(json_stuff)):\n",
    "        # if key is in the set of keys, add to the mining_list\n",
    "        if json_stuff[x]['key'] not in set_key: # this probably needs to be updated..? i don't know anymore\n",
    "            if json_stuff[x]['performed_by'] == id_1:\n",
    "                mining_list.append(json_stuff[x])\n",
    "                print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "\n",
    "        # Otherwise, break the loop\n",
    "        else:\n",
    "            print('All data has already been extracted')\n",
    "            break_point = True\n",
    "            break # Add this line and indent for subsequent run only\n",
    "\n",
    "    # Get out of the data request if break_point is True\n",
    "    if break_point:\n",
    "        print(f'No more new records to be processed. Total Loops: {n}')\n",
    "        break\n",
    "    \n",
    "    # Otherwise keep going\n",
    "    offset = response_json['next_offset']\n",
    "    \n",
    "    # It will never get to -1 since loop will break once the same record is id'ed, remove?\n",
    "#     if offset == -1:\n",
    "#         print(f'No more records to be processed. Total Loops: {n}')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Infinite loop until the list is empty (offset ran out)\n",
    "\n",
    "# while True:\n",
    "#     response = requests.get(f'{user_2}{offset}')\n",
    "#     # Get out of the loop if request is unsuccessful\n",
    "#     if response.status_code != 200:\n",
    "#         print('Unsuccessful Request T^T')\n",
    "#         break\n",
    "#     response_json = response.json()\n",
    "#     json_stuff = response_json['list']\n",
    "    \n",
    "#     # If the list is empty in the json data\n",
    "#     if not json_stuff:\n",
    "#         break\n",
    "    \n",
    "#     for x in range(0,len(json_stuff)):\n",
    "#         if json_stuff[x]['performed_by'] == id_1:\n",
    "#             key.append(json_stuff[x]['key'])\n",
    "#             inviter.append(json_stuff[x]['performed_by'])\n",
    "#             title.append(json_stuff[x]['title'])\n",
    "#             artist.append(json_stuff[x]['artist'])\n",
    "#             date.append(json_stuff[x]['created_at'])\n",
    "#             recording_url = recording_base_url + json_stuff[x]['web_url']\n",
    "#             song_url.append(recording_url)\n",
    "#             print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "            \n",
    "#     offset = response_json['next_offset']\n",
    "    \n",
    "#     if offset == -1:\n",
    "#         print('No more records to be processed')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapped all the wanted field from the data mining + rename field\n",
    "def create_song_list(track):\n",
    "    key = track['key']\n",
    "    title = track['title']\n",
    "    artist = track['artist']\n",
    "    date = track['created_at']\n",
    "    inviter = track['performed_by']\n",
    "    recording_base_url = 'https://www.smule.com'\n",
    "    recording_url = recording_base_url + track['web_url']\n",
    "    \n",
    "    return {\n",
    "        'key': key,\n",
    "        'Title': title,\n",
    "        'Artist': artist,\n",
    "        'Date_Created': date,\n",
    "        'Invite_Spawner': inviter,\n",
    "        'URL': recording_url\n",
    "    }\n",
    "    \n",
    "# Use map to give a list iterator\n",
    "song_list_iterator = map(create_song_list, mining_list)\n",
    "# Turn iterator into a list\n",
    "song_list = list(song_list_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To dataframe and transform date string\n",
    "id2_songs_add_df = pd.DataFrame(song_list)\n",
    "id2_songs_add_df['Date_Created'] = id2_songs_add_df['Date_Created'].str.split('T').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save first result to csv\n",
    "id2_songs_add_df.to_csv('../data/id2_songs_add.csv', encoding='utf_8_sig', index=False)\n",
    "id2_songs_add_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_2 is the VIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the above mapping function works, overhaul this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run another infinite loop again - You might have to wait 30mins b/c you'll get 418 code :P\n",
    "\n",
    "while True:\n",
    "    response = requests.get(f'{user_1}{offset}')\n",
    "    # Get out of the loop if request is unsuccessful\n",
    "    if response.status_code != 200:\n",
    "        print('Unsuccessful Request T^T')\n",
    "        break\n",
    "    response_json = response.json()\n",
    "    json_stuff = response_json['list']\n",
    "    \n",
    "    # If the list is empty in the json data\n",
    "    if not json_stuff:\n",
    "        break\n",
    "    \n",
    "    for x in range(0,len(json_stuff)):\n",
    "#         if json_stuff[x]['key'] not in set_key: # Add this line and indent for subsequent run only\n",
    "        if json_stuff[x]['performed_by'] == id_2:\n",
    "            key.append(json_stuff[x]['key'])\n",
    "            inviter.append(json_stuff[x]['performed_by'])\n",
    "            title.append(json_stuff[x]['title'])\n",
    "            artist.append(json_stuff[x]['artist'])\n",
    "            date.append(json_stuff[x]['created_at'])\n",
    "            recording_url = recording_base_url + json_stuff[x]['web_url']\n",
    "            song_url.append(recording_url)\n",
    "            print(f'Processing record index #{offset + x}: {offset}, #{x}')\n",
    "#         else: # Add this line and indent for subsequent run only\n",
    "#             print('All data has already been extracted') # Add this line and indent for subsequent run only\n",
    "#             break # Add this line and indent for subsequent run only\n",
    "    offset = response_json['next_offset']\n",
    "    \n",
    "    if offset == -1:\n",
    "        print('No more records to be processed')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe and save the first data request into csv\n",
    "date_created = []\n",
    "\n",
    "for d in date:\n",
    "    date_only = d.split('T')[0]\n",
    "    date_created.append(date_only)\n",
    "\n",
    "# Show the list of songs in dataframe for id_2\n",
    "id1_songs_add_df = pd.DataFrame({'key': key,\n",
    "                             'Date_Created': date_created,\n",
    "                             'Title':title,\n",
    "                             'Artist': artist,\n",
    "                             'Invite_Spawner': inviter,\n",
    "                             'URL': song_url})\n",
    "\n",
    "# Save first result to csv\n",
    "id1_songs_add_df.to_csv('../data/id1_songs_add.csv', encoding='utf_8_sig', index=False)\n",
    "id1_songs_add_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the two csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import id2_songs.csv\n",
    "id2_songs_add_df = pd.read_csv('../data/id2_songs_add.csv')\n",
    "id2_songs_add_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHEN DATA IS RETRIEVED, CHECK IF THERE'S EXTRA INDEX AND REMOVE OR DO WHATEVER NECESSARY TO APPEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the two dataframes\n",
    "all_songs_df = id1_songs_add_df.append(id2_songs_add_df)\n",
    "\n",
    "# Append to the original dataframe\n",
    "all_songs_df = all_songs_df.append(former_song_df)\n",
    "\n",
    "# Sort the df by date\n",
    "all_songs_df = all_songs_df.sort_values('Date_Created', ascending=False)\n",
    "\n",
    "# Reset index\n",
    "all_songs_df.reset_index(inplace=True)\n",
    "\n",
    "# Delete old index\n",
    "del all_songs_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result\n",
    "print(f'{id_1} joined {id_2} on {len(id1_songs_add_df)} additional songs!')\n",
    "print(f'{id_2} joined {id_1} on {len(id2_songs_add_df)} additional songs!')\n",
    "print(f'You guys have made {len(all_songs_df)} recordings together as of this date.')\n",
    "\n",
    "all_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv with encoding that allows foreign character\n",
    "# all_songs_df.to_csv('../data/ochalady-sakusaku.csv', encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only if both runs go smoothly, run the code below and skip above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up & Export -- THIS NEEDS TO BE CLEANED UP -- ALSO LET BE REAL, WE NEVER GONNA BE ABLE TO DO THIS CUZ THE D*** TEAPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the date column\n",
    "# date_created = []\n",
    "\n",
    "# for d in date:\n",
    "#     date_only = d.split('T')[0]\n",
    "#     date_created.append(date_only)\n",
    "    \n",
    "# # Show the list of songs in dataframe\n",
    "# all_songs_df = pd.DataFrame({'key': key,\n",
    "#                              'Date_Created': date_created,\n",
    "#                              'Title':title,\n",
    "#                              'Artist': artist,\n",
    "#                              'Invite_Spawner': inviter,\n",
    "#                              'URL': song_url})\n",
    "\n",
    "# # Sort the df by date\n",
    "# all_songs_df = all_songs_df.sort_values('Date Created')\n",
    "\n",
    "# # Reset index\n",
    "# all_songs_df.reset_index(inplace=True)\n",
    "\n",
    "# # Delete old index\n",
    "# del all_songs_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ta..da...\n",
    "print(f'{id_1} joined {id_2} on {len(id1_songs_add_df)} additional songs!')\n",
    "print(f'{id_2} joined {id_1} on {len(id2_songs_add_df)} additional songs!')\n",
    "print(f'You guys have made {len(all_songs_df)} recordings together as of this date.')\n",
    "\n",
    "all_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv with encoding that allows foreign character\n",
    "# all_songs_df.to_csv('ochalady-sakusaku.csv', encoding='utf_8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
